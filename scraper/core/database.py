"""
Database utilities for scraper.
Provides async connection pool and CRUD operations for listings.
"""
import asyncpg
import logging
from typing import Optional, Dict, Any, List
from uuid import UUID, uuid4
from datetime import datetime
from contextlib import asynccontextmanager

from .filters import get_filter_manager

logger = logging.getLogger(__name__)


class DatabaseManager:
    """Manages PostgreSQL connection pool for scraper."""
    
    def __init__(self, host: str, port: int, database: str, user: str, password: str, 
                 min_size: int = 5, max_size: int = 20):
        self.host = host
        self.port = port
        self.database = database
        self.user = user
        self.password = password
        self.min_size = min_size
        self.max_size = max_size
        self._pool: Optional[asyncpg.Pool] = None
    
    async def connect(self) -> None:
        """Create connection pool."""
        if self._pool is not None:
            logger.warning("Database pool already exists")
            return
        
        try:
            self._pool = await asyncpg.create_pool(
                host=self.host,
                port=self.port,
                database=self.database,
                user=self.user,
                password=self.password,
                min_size=self.min_size,
                max_size=self.max_size,
            )
            logger.info(f"Database pool connected to {self.host}:{self.port}/{self.database}")
        except Exception as exc:
            logger.exception(f"Failed to create database pool: {exc}")
            raise
    
    async def disconnect(self) -> None:
        """Close connection pool."""
        if self._pool is not None:
            await self._pool.close()
            self._pool = None
            logger.info("Database pool closed")
    
    @asynccontextmanager
    async def acquire(self):
        """Acquire connection from pool."""
        if self._pool is None:
            raise RuntimeError("Database pool not initialized. Call connect() first.")
        
        async with self._pool.acquire() as conn:
            yield conn
    
    async def get_source_by_code(self, source_code: str) -> Optional[Dict[str, Any]]:
        """
        ZÃ­skÃ¡ source (zdroj) podle kÃ³du.
        
        Args:
            source_code: KÃ³d zdroje (napÅ™. "REMAX", "MMR")
            
        Returns:
            Dict se source daty nebo None
        """
        async with self.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT id, code, name, base_url, is_active
                FROM re_realestate.sources
                WHERE code = $1
                """,
                source_code
            )
            if row:
                return dict(row)
            return None
    
    async def upsert_listing(self, listing_data: Dict[str, Any]) -> Optional[UUID]:
        """
        Upsert listing do databÃ¡ze (atomicky bez race condition).
        
        Pokud listing s danÃ½m (source_id, external_id) jiÅ¾ existuje, aktualizuje ho.
        Pokud neexistuje, vytvoÅ™Ã­ novÃ½.
        
        PouÅ¾Ã­vÃ¡ PostgreSQL ON CONFLICT DO UPDATE pattern - je atomickÃ¡ a bezpeÄnÃ¡
        i pÅ™i soubÄ›Å¾nÃ½ch insertÅ¯ se stejnÃ½m external_id.
        
        Kontroluje searchovacÃ­ filtry - pokud inzerÃ¡t nedodpovÃ­dÃ¡ kritÃ©riÃ­m,
        nebude vloÅ¾en do DB.
        
        Args:
            listing_data: Dictionary s daty listingu
            
        Returns:
            UUID listingu (novÃ©ho nebo existujÃ­cÃ­ho) nebo None pokud je vylouÄen filtry
        """
        # ğŸ”¥ Kontrola filtrÅ¯
        filter_mgr = get_filter_manager()
        should_include, exclusion_reason = filter_mgr.should_include_listing(listing_data)
        
        if not should_include:
            filter_mgr.log_listing_decision(listing_data, False, exclusion_reason)
            logger.debug(f"Skipped listing due to filter: {exclusion_reason}")
            return None
        
        # ZÃ­skej source_id podle source_code
        source = await self.get_source_by_code(listing_data["source_code"])
        if not source:
            raise ValueError(f"Source '{listing_data['source_code']}' not found in database")
        
        source_id = source["id"]
        source_name = source["name"]
        external_id = listing_data.get("external_id")
        listing_id = uuid4()
        
        # MapovÃ¡nÃ­ ÄeskÃ½ch hodnot na enum hodnoty v DB
        property_type_map = {
            "DÅ¯m": "House",
            "Byt": "Apartment",
            "Pozemek": "Land",
            "Chata": "Cottage",
            "KomerÄnÃ­": "Commercial",
            "PrÅ¯myslovÃ½": "Industrial",
            "GarÃ¡Å¾": "Garage",
            "OstatnÃ­": "Other",
        }
        
        offer_type_map = {
            "Prodej": "Sale",
            "PronÃ¡jem": "Rent",
        }
        
        property_type_db = property_type_map.get(listing_data.get("property_type", "OstatnÃ­"), "Other")
        offer_type_db = offer_type_map.get(listing_data.get("offer_type", "Prodej"), "Sale")
        
        now = datetime.utcnow()
        
        async with self.acquire() as conn:
            # ğŸ”¥ ATOMIC UPSERT s ON CONFLICT DO UPDATE
            # Å½Ã¡dnÃ© race conditions - DB se postarÃ¡ o atomicitu
            result = await conn.fetchval(
                """
                INSERT INTO re_realestate.listings (
                    id, source_id, source_code, source_name, external_id, url,
                    title, description, property_type, offer_type, price,
                    location_text, area_built_up, area_land,
                    first_seen_at, last_seen_at, is_active
                )
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, true)
                ON CONFLICT (source_id, external_id) DO UPDATE
                SET
                    url = EXCLUDED.url,
                    title = EXCLUDED.title,
                    description = EXCLUDED.description,
                    property_type = EXCLUDED.property_type,
                    offer_type = EXCLUDED.offer_type,
                    price = EXCLUDED.price,
                    location_text = EXCLUDED.location_text,
                    area_built_up = EXCLUDED.area_built_up,
                    area_land = EXCLUDED.area_land,
                    last_seen_at = EXCLUDED.last_seen_at,
                    is_active = true
                RETURNING id
                """,
                listing_id,
                source_id,
                listing_data["source_code"],
                source_name,
                external_id,
                listing_data.get("url", ""),
                listing_data.get("title", "")[:200],
                listing_data.get("description", "")[:5000],
                property_type_db,
                offer_type_db,
                listing_data.get("price"),
                listing_data.get("location_text", "")[:200],
                listing_data.get("area_built_up"),
                listing_data.get("area_land"),
                now,
                now
            )
            
            # Pokud UPDATE navrÃ¡til existujÃ­cÃ­ ID, pouÅ¾ij to
            final_listing_id = result if result else listing_id
            
            # Synchronizuj fotky v transakci
            if "photos" in listing_data and listing_data["photos"]:
                await self._upsert_photos(conn, final_listing_id, listing_data["photos"])
            
            logger.debug(f"Upserted listing {final_listing_id} (external_id={external_id})")
            return final_listing_id
    
    async def _upsert_photos(self, conn, listing_id: UUID, photo_urls: List[str]) -> None:
        """
        Upsert fotek pro listing.
        
        SmaÅ¾e starÃ© fotky a vloÅ¾Ã­ novÃ©.
        WICHTIG: BÄ›Å¾Ã­ v transakci, aby DELETE+INSERT byly atomickÃ©.
        """
        async with conn.transaction():
            # Smazat starÃ© fotky
            await conn.execute(
                "DELETE FROM re_realestate.listing_photos WHERE listing_id = $1",
                listing_id
            )
            
            # VloÅ¾it novÃ© fotky
            for idx, photo_url in enumerate(photo_urls[:20]):  # Max 20 photos
                photo_id = uuid4()
                await conn.execute(
                    """
                    INSERT INTO re_realestate.listing_photos (
                        id, listing_id, original_url, order_index, created_at
                    )
                    VALUES ($1, $2, $3, $4, $5)
                    """,
                    photo_id,
                    listing_id,
                    photo_url,
                    idx,
                    datetime.utcnow()
                )
        
        logger.debug(f"Upserted {len(photo_urls)} photos for listing {listing_id}")
    
    # ============================================================================
    # Scrape Jobs Persistence
    # ============================================================================
    
    async def create_scrape_job(self, job_id: UUID, source_codes: List[str], 
                                full_rescan: bool = False) -> None:
        """
        VytvoÅ™Ã­ zÃ¡znam scrape jobu v databÃ¡zi.
        
        Args:
            job_id: UUID jobu
            source_codes: List zdrojÅ¯ k scrapovÃ¡nÃ­
            full_rescan: true pro full rescan, false pro incrementÃ¡lnÃ­
        """
        async with self.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO re_realestate.scrape_jobs (
                    id, source_codes, full_rescan, status, progress, created_at
                )
                VALUES ($1, $2, $3, $4, $5, $6)
                """,
                job_id,
                source_codes,  # asyncpg automaticky konvertuje list na PostgreSQL array
                full_rescan,
                'Queued',
                0,
                datetime.utcnow()
            )
            logger.info(f"Created scrape job {job_id} for sources: {source_codes}")
    
    async def update_scrape_job(self, job_id: UUID, status: str, 
                               progress: int = None, error_message: str = None,
                               listings_found: int = None, listings_new: int = None,
                               listings_updated: int = None, started_at: datetime = None,
                               finished_at: datetime = None) -> None:
        """
        Aktualizuje scrape job s novÃ½mi daty.
        
        Args:
            job_id: UUID jobu
            status: NovÃ½ status
            progress: Progress 0-100
            error_message: ChybovÃ¡ zprÃ¡va
            listings_found: PoÄet nalezenÃ½ch inzerÃ¡tÅ¯
            listings_new: PoÄet novÃ½ch inzerÃ¡tÅ¯
            listings_updated: PoÄet aktualizovanÃ½ch inzerÃ¡tÅ¯
            started_at: ÄŒas startu jobu
            finished_at: ÄŒas ukonÄenÃ­ jobu
        """
        updates = []
        params = []
        param_idx = 1
        
        # Build dynamic UPDATE query
        if status is not None:
            updates.append(f"status = ${param_idx}")
            params.append(status)
            param_idx += 1
        
        if progress is not None:
            updates.append(f"progress = ${param_idx}")
            params.append(progress)
            param_idx += 1
        
        if error_message is not None:
            updates.append(f"error_message = ${param_idx}")
            params.append(error_message)
            param_idx += 1
        
        if listings_found is not None:
            updates.append(f"listings_found = ${param_idx}")
            params.append(listings_found)
            param_idx += 1
        
        if listings_new is not None:
            updates.append(f"listings_new = ${param_idx}")
            params.append(listings_new)
            param_idx += 1
        
        if listings_updated is not None:
            updates.append(f"listings_updated = ${param_idx}")
            params.append(listings_updated)
            param_idx += 1
        
        if started_at is not None:
            updates.append(f"started_at = ${param_idx}")
            params.append(started_at)
            param_idx += 1
        
        if finished_at is not None:
            updates.append(f"finished_at = ${param_idx}")
            params.append(finished_at)
            param_idx += 1
        
        if not updates:
            return
        
        # PÅ™idej job_id jako poslednÃ­ parametr
        params.append(job_id)
        
        query = f"""
            UPDATE re_realestate.scrape_jobs
            SET {', '.join(updates)}
            WHERE id = ${param_idx}
        """
        
        async with self.acquire() as conn:
            await conn.execute(query, *params)
            logger.debug(f"Updated scrape job {job_id}: {', '.join(updates)}")
    
    async def get_scrape_job(self, job_id: UUID) -> Optional[Dict[str, Any]]:
        """
        NaÄte scrape job z databÃ¡ze.
        
        Args:
            job_id: UUID jobu
            
        Returns:
            Dict se scrape job daty nebo None
        """
        async with self.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT id, source_codes, full_rescan, status, progress,
                       listings_found, listings_new, listings_updated,
                       error_message, created_at, started_at, finished_at
                FROM re_realestate.scrape_jobs
                WHERE id = $1
                """,
                job_id
            )
            if row:
                return dict(row)
            return None
    
    async def list_scrape_jobs(self, limit: int = 50, status: str = None) -> List[Dict[str, Any]]:
        """
        VypÃ­Å¡e scrape joby seÅ™azenÃ© chronologicky (nejnovÄ›jÅ¡Ã­ prvnÃ­).
        
        Args:
            limit: MaximÃ¡lnÃ­ poÄet jobÅ¯
            status: Filtr podle statusu (napÅ™. "Queued", "Running", "Succeeded")
            
        Returns:
            List scrape jobÅ¯
        """
        query = """
            SELECT id, source_codes, full_rescan, status, progress,
                   listings_found, listings_new, listings_updated,
                   error_message, created_at, started_at, finished_at
            FROM re_realestate.scrape_jobs
        """
        
        if status:
            query += f" WHERE status = '{status}'"
        
        query += " ORDER BY created_at DESC LIMIT $1"
        
        async with self.acquire() as conn:
            rows = await conn.fetch(query, limit)
            return [dict(row) for row in rows]


# GlobÃ¡lnÃ­ instance (singleton pattern)
_db_manager: Optional[DatabaseManager] = None


def get_db_manager() -> DatabaseManager:
    """ZÃ­skÃ¡ globÃ¡lnÃ­ instanci DatabaseManager."""
    global _db_manager
    if _db_manager is None:
        raise RuntimeError("DatabaseManager not initialized. Call init_db_manager() first.")
    return _db_manager


def init_db_manager(host: str, port: int, database: str, user: str, password: str,
                   min_size: int = 5, max_size: int = 20) -> DatabaseManager:
    """Inicializuje globÃ¡lnÃ­ DatabaseManager."""
    global _db_manager
    _db_manager = DatabaseManager(host, port, database, user, password, min_size, max_size)
    return _db_manager
