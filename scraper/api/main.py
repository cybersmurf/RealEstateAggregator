"""
FastAPI application for Real Estate Scraper.
Provides REST endpoints to trigger and monitor scraping jobs.
"""
from contextlib import asynccontextmanager
from fastapi import FastAPI, BackgroundTasks, HTTPException
from uuid import uuid4, UUID
from datetime import datetime
import yaml
from pathlib import Path
import os
import logging

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

from .schemas import ScrapeTriggerRequest, ScrapeTriggerResponse, ScrapeJob
from core.runner import run_scrape_job
from core.database import init_db_manager, get_db_manager
from core.geocoding import bulk_geocode, geocode_address
from core.ruian_service import lookup_ruian_address, bulk_ruian_lookup

logger = logging.getLogger(__name__)

# GlobÃ¡lnÃ­ scheduler instance
_scheduler: AsyncIOScheduler | None = None

def get_scheduler() -> AsyncIOScheduler:
    if _scheduler is None:
        raise RuntimeError("Scheduler nenÃ­ inicializovÃ¡n")
    return _scheduler


async def _scheduled_scrape_all(full_rescan: bool = False) -> None:
    """NaplÃ¡novanÃ½ scraping vÅ¡ech aktivnÃ­ch zdrojÅ¯."""
    db_manager = get_db_manager()

    async with db_manager.acquire() as conn:
        rows = await conn.fetch(
            "SELECT code FROM re_realestate.sources WHERE is_active = true ORDER BY code"
        )
    source_codes = [r["code"] for r in rows]

    if not source_codes:
        logger.warning("[Scheduler] Å½Ã¡dnÃ© aktivnÃ­ zdroje k scrapovÃ¡nÃ­")
        return

    job_id = uuid4()
    request = ScrapeTriggerRequest(source_codes=source_codes, full_rescan=full_rescan)
    await db_manager.create_scrape_job(job_id=job_id, source_codes=source_codes, full_rescan=full_rescan)
    logger.info(f"[Scheduler] SpouÅ¡tÃ­m naplÃ¡novanÃ½ scraping: {len(source_codes)} zdrojÅ¯, job_id={job_id}, full_rescan={full_rescan}")
    await run_scrape_job(job_id, request)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI lifespan context manager.
    Handles startup and shutdown events.
    
    Replaces deprecated @app.on_event("startup"/"shutdown") decorators.
    """
    # ============================================================================
    # STARTUP
    # ============================================================================
    try:
        # NaÄti config ze settings.yaml
        config_path = Path(__file__).parent.parent / "config" / "settings.yaml"
        
        if not config_path.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)
        
        if not config:
            raise ValueError("Empty config file")
        
        # Inicializuj DB manager
        db_config = config.get("database", {})
        
        if not db_config:
            raise ValueError("Missing 'database' section in config")
        
        db_config = {
            **db_config,
            "host": os.getenv("DB_HOST", db_config.get("host", "localhost")),
            "port": int(os.getenv("DB_PORT", db_config.get("port", 5432))),
            "database": os.getenv("DB_NAME", db_config.get("database", "realestate_dev")),
            "user": os.getenv("DB_USER", db_config.get("user", "postgres")),
            "password": os.getenv("DB_PASSWORD", db_config.get("password", "dev")),
        }

        db_manager = init_db_manager(
            host=db_config.get("host"),
            port=db_config.get("port"),
            database=db_config.get("database"),
            user=db_config.get("user"),
            password=db_config.get("password"),
            min_size=db_config.get("min_connections", 5),
            max_size=db_config.get("max_connections", 20),
            source_cache_ttl_seconds=db_config.get("source_cache_ttl_seconds", 3600),
        )
        
        # PÅ™ipoj k databÃ¡zi
        await db_manager.connect()
        logger.info(f"âœ“ Database connected to {db_config.get('host')}:{db_config.get('port')}/{db_config.get('database')}")
        print(f"âœ“ Database connected to {db_config.get('host')}:{db_config.get('port')}/{db_config.get('database')}")

        # â”€â”€ Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        global _scheduler
        _scheduler = AsyncIOScheduler(timezone="Europe/Prague")

        sched_cfg = config.get("scheduler", {})
        if sched_cfg.get("enabled", True):
            # DennÃ­ quick scraping â€“ kaÅ¾dÃ½ den ve 3:00 rÃ¡no
            daily_cron = sched_cfg.get("daily_cron", "0 3 * * *")
            _scheduler.add_job(
                _scheduled_scrape_all,
                CronTrigger.from_crontab(daily_cron, timezone="Europe/Prague"),
                id="daily_scrape",
                name="DennÃ­ scraping (vÅ¡echny zdroje)",
                replace_existing=True,
                kwargs={"full_rescan": False},
            )
            # TÃ½dennÃ­ full rescan â€“ v nedÄ›li v 2:00
            weekly_cron = sched_cfg.get("weekly_cron", "0 2 * * 0")
            _scheduler.add_job(
                _scheduled_scrape_all,
                CronTrigger.from_crontab(weekly_cron, timezone="Europe/Prague"),
                id="weekly_full_rescan",
                name="TÃ½dennÃ­ full rescan",
                replace_existing=True,
                kwargs={"full_rescan": True},
            )
            _scheduler.start()
            logger.info(f"âœ“ Scheduler spuÅ¡tÄ›n | dennÃ­={daily_cron} | weekly={weekly_cron}")
            print(f"âœ“ Scheduler spuÅ¡tÄ›n | dennÃ­={daily_cron} | tÃ½dennÃ­={weekly_cron}")
        else:
            logger.info("Scheduler je vypnut (scheduler.enabled=false v settings.yaml)")

    except Exception as exc:
        logger.error(f"âŒ Startup failed: {exc}")
        print(f"âŒ Startup failed: {exc}")
        raise
    
    # Yield control to FastAPI app
    yield
    
    # ============================================================================
    # SHUTDOWN
    # ============================================================================
    try:
        if _scheduler and _scheduler.running:
            _scheduler.shutdown(wait=False)
            logger.info("âœ“ Scheduler zastaven")
        db_manager = get_db_manager()
        await db_manager.disconnect()
        logger.info("âœ“ Database disconnected")
        print("âœ“ Database disconnected")
    except Exception as exc:
        logger.error(f"Error during shutdown: {exc}")


app = FastAPI(
    title="RealEstate Scraper API",
    description="API pro spouÅ¡tÄ›nÃ­ a monitorovÃ¡nÃ­ scraping jobÅ¯ realitnÃ­ch inzerÃ¡tÅ¯",
    version="1.0.0",
    lifespan=lifespan  # ğŸ”¥ NovÃ½ pÅ™Ã­stup - lifespan context manager
)


@app.get("/")
async def root():
    """Health check endpoint."""
    return {
        "service": "RealEstate Scraper API",
        "status": "running",
        "version": "1.0.0"
    }


@app.post("/v1/scrape/run", response_model=ScrapeTriggerResponse)
async def trigger_scrape(
    request: ScrapeTriggerRequest,
    background_tasks: BackgroundTasks,
) -> ScrapeTriggerResponse:
    """
    SpustÃ­ scraping job v pozadÃ­.
    
    Args:
        request: ScrapeTriggerRequest s optional source_codes a full_rescan flag
        background_tasks: FastAPI BackgroundTasks pro async spuÅ¡tÄ›nÃ­
        
    Returns:
        ScrapeTriggerResponse s job_id a statusem
    """
    job_id = uuid4()
    
    # VytvoÅ™ DB record pro job
    db_manager = get_db_manager()
    await db_manager.create_scrape_job(
        job_id=job_id,
        source_codes=request.source_codes or [],
        full_rescan=request.full_rescan
    )

    # Spustit async v backgroundu, aby API odpovÄ›dÄ›lo hned
    background_tasks.add_task(run_scrape_job, job_id, request)

    return ScrapeTriggerResponse(
        job_id=job_id,
        status="Queued",
        message="Scraping job enqueued.",
    )


@app.get("/v1/scrape/jobs/{job_id}", response_model=ScrapeJob)
async def get_scrape_job(job_id: str) -> ScrapeJob:
    """
    ZÃ­skÃ¡ status scraping jobu podle job_id.
    
    Args:
        job_id: UUID jobu jako string
        
    Returns:
        ScrapeJob s aktuÃ¡lnÃ­m stavem
        
    Raises:
        HTTPException 404 pokud job neexistuje
    """
    try:
        job_uuid = UUID(job_id)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid job_id format")
    
    db_manager = get_db_manager()
    job_data = await db_manager.get_scrape_job(job_uuid)
    
    if not job_data:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    # PÅ™ekonvertuj DB data na ScrapeJob Pydantic model
    return ScrapeJob(
        job_id=job_data['id'],
        source_codes=list(job_data['source_codes']),  # PostgreSQL array â†’ Python list
        full_rescan=job_data['full_rescan'],
        created_at=job_data['created_at'],
        started_at=job_data['started_at'],
        finished_at=job_data['finished_at'],
        status=job_data['status'],
        progress=job_data['progress'],
        error_message=job_data['error_message'],
    )


@app.get("/v1/scrape/jobs", response_model=list[ScrapeJob])
async def list_scrape_jobs(limit: int = 50, status: str = None) -> list[ScrapeJob]:
    """
    VrÃ¡tÃ­ seznam scraping jobÅ¯ seÅ™azenÃ½ch chronologicky (nejnovÄ›jÅ¡Ã­ prvnÃ­).
    
    Args:
        limit: MaximÃ¡lnÃ­ poÄet jobÅ¯ k vrÃ¡cenÃ­ (default 50)
        status: VolitelnÃ½ filtr podle statusu (Queued, Running, Succeeded, Failed)
    
    Returns:
        List[ScrapeJob] - joby z databÃ¡ze
    """
    db_manager = get_db_manager()
    jobs_data = await db_manager.list_scrape_jobs(limit=limit, status=status)
    
    return [
        ScrapeJob(
            job_id=job['id'],
            source_codes=list(job['source_codes']),
            full_rescan=job['full_rescan'],
            created_at=job['created_at'],
            started_at=job['started_at'],
            finished_at=job['finished_at'],
            status=job['status'],
            progress=job['progress'],
            error_message=job['error_message'],
        )
        for job in jobs_data
    ]


# ============================================================================
# GEOCODING ENDPOINTS
# ============================================================================

@app.post("/v1/geocode/bulk")
async def trigger_bulk_geocode(background_tasks: BackgroundTasks, batch_size: int = 50):
    """SpustÃ­ dÃ¡vkovÃ© geokÃ³dovÃ¡nÃ­ inzerÃ¡tÅ¯ bez GPS souÅ™adnic pomocÃ­ Nominatim."""
    batch_size = min(max(1, batch_size), 200)
    db_manager = get_db_manager()

    async def _run_geocode():
        count = await bulk_geocode(db_manager, batch_size=batch_size)
        logger.info(f"Bulk geocoding dokonÄen: {count} inzerÃ¡tÅ¯ geokÃ³dovÃ¡no")

    background_tasks.add_task(_run_geocode)
    return {"status": "started", "batch_size": batch_size, "message": "Geocoding bÄ›Å¾Ã­ na pozadÃ­"}


@app.get("/v1/geocode/single")
async def geocode_single(address: str, country: str = "CZ"):
    """GeokÃ³duje jedinou adresu pomocÃ­ Nominatim â€“ pro testovÃ¡nÃ­."""
    result = await geocode_address(address, country=country)
    if result:
        lat, lon = result
        return {"latitude": lat, "longitude": lon, "address": address}
    raise HTTPException(status_code=404, detail=f"Adresa nenalezena: '{address}'")


@app.get("/v1/geocode/stats")
async def geocode_stats():
    """Statistika geokÃ³dovÃ¡nÃ­ â€“ kolik inzerÃ¡tÅ¯ mÃ¡ / nemÃ¡ GPS souÅ™adnice."""
    db_manager = get_db_manager()
    async with db_manager.acquire() as conn:
        row = await conn.fetchrow(
            """
            SELECT
                COUNT(*) AS total,
                COUNT(*) FILTER (WHERE latitude IS NOT NULL) AS with_coords,
                COUNT(*) FILTER (WHERE latitude IS NULL AND is_active = true) AS active_without_coords,
                COUNT(*) FILTER (WHERE geocode_source = 'scraper') AS from_scraper,
                COUNT(*) FILTER (WHERE geocode_source = 'nominatim') AS from_nominatim
            FROM re_realestate.listings
            """
        )
    return {
        "total": row["total"],
        "with_coords": row["with_coords"],
        "active_without_coords": row["active_without_coords"],
        "from_scraper": row["from_scraper"],
        "from_nominatim": row["from_nominatim"],
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÄŒÃšZK / RUIAN â€“ Katastr nemovitostÃ­
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/v1/ruian/single")
async def ruian_single(address: str, municipality: str = ""):
    """
    VyhledÃ¡ adresu v RUIAN a vrÃ¡tÃ­ kÃ³d adresnÃ­ho mÃ­sta + odkaz na nahlÃ­Å¾enÃ­ do KN.
    Parametry:
      address    â€“ adresa (nebo jen nÃ¡zev obce)
      municipality â€“ nÃ¡zev obce (preferovanÃ½ vstup, pÅ™esnÄ›jÅ¡Ã­ vÃ½sledky)
    """
    result = await lookup_ruian_address(address, municipality=municipality or None)
    return result


@app.post("/v1/ruian/bulk")
async def ruian_bulk(
    background_tasks: BackgroundTasks,
    batch_size: int = 50,
    overwrite_not_found: bool = False,
):
    """
    HromadnÃ© vyhledÃ¡vÃ¡nÃ­ RUIAN pro inzerÃ¡ty bez katastrÃ¡lnÃ­ch dat.
    VÃ½sledky se uklÃ¡dajÃ­ do tabulky listing_cadastre_data.
    âš ï¸ PomalÃ© kvÅ¯li rate limitingu (1 req/s). BÄ›Å¾Ã­ na pozadÃ­.
    """
    db_manager = get_db_manager()
    batch_size = min(max(batch_size, 1), 200)

    async def _run():
        stats = await bulk_ruian_lookup(
            db_manager,
            batch_size=batch_size,
            overwrite_not_found=overwrite_not_found,
        )
        logger.info(f"RUIAN bulk hotovo: {stats}")

    background_tasks.add_task(_run)
    return {
        "status": "started",
        "batch_size": batch_size,
        "overwrite_not_found": overwrite_not_found,
        "message": "RUIAN bulk lookup bÄ›Å¾Ã­ na pozadÃ­",
    }


@app.get("/v1/ruian/stats")
async def ruian_stats():
    """Statistika RUIAN / katastrÃ¡lnÃ­ch dat v DB."""
    db_manager = get_db_manager()
    async with db_manager.acquire() as conn:
        row = await conn.fetchrow(
            """
            SELECT
                COUNT(*) AS total_listings,
                COUNT(lcd.id) AS with_cadastre_data,
                COUNT(lcd.id) FILTER (WHERE lcd.fetch_status = 'found') AS ruian_found,
                COUNT(lcd.id) FILTER (WHERE lcd.fetch_status = 'not_found') AS ruian_not_found,
                COUNT(lcd.id) FILTER (WHERE lcd.fetch_status = 'error') AS ruian_error,
                COUNT(lcd.id) FILTER (WHERE lcd.fetch_status = 'manual') AS manual
            FROM re_realestate.listings l
            LEFT JOIN re_realestate.listing_cadastre_data lcd ON lcd.listing_id = l.id
            WHERE l.is_active = true
            """
        )

    return {
        "total_active_listings": row["total_listings"],
        "with_cadastre_data": row["with_cadastre_data"],
        "ruian_found": row["ruian_found"],
        "ruian_not_found": row["ruian_not_found"],
        "ruian_error": row["ruian_error"],
        "manual": row["manual"],
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCHEDULER â€“ sprÃ¡va naplÃ¡novanÃ½ch Ãºloh
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/v1/schedule/jobs")
async def list_scheduled_jobs():
    """VrÃ¡tÃ­ seznam naplÃ¡novanÃ½ch Ãºloh a jejich pÅ™Ã­Å¡tÃ­ spuÅ¡tÄ›nÃ­."""
    if _scheduler is None:
        return {"enabled": False, "jobs": []}

    jobs = []
    for job in _scheduler.get_jobs():
        jobs.append({
            "id": job.id,
            "name": job.name,
            "next_run": job.next_run_time.isoformat() if job.next_run_time else None,
            "trigger": str(job.trigger),
        })
    return {
        "enabled": _scheduler.running,
        "timezone": "Europe/Prague",
        "jobs": jobs,
    }


@app.post("/v1/schedule/trigger-now")
async def trigger_scheduled_scrape_now(
    background_tasks: BackgroundTasks,
    full_rescan: bool = False,
):
    """
    OkamÅ¾itÄ› spustÃ­ naplÃ¡novanÃ½ scraping (stejnÄ› jako by ho pustil scheduler).
    UÅ¾iteÄnÃ© pro ruÄnÃ­ spuÅ¡tÄ›nÃ­ bez ÄekÃ¡nÃ­ na plÃ¡novanÃ½ Äas.
    """
    background_tasks.add_task(_scheduled_scrape_all, full_rescan)
    return {
        "status": "started",
        "full_rescan": full_rescan,
        "message": f"NaplÃ¡novanÃ½ scraping spuÅ¡tÄ›n okamÅ¾itÄ› (full_rescan={full_rescan})",
    }


@app.put("/v1/schedule/jobs/{job_id}/pause")
async def pause_scheduled_job(job_id: str):
    """PozastavÃ­ naplÃ¡novanou Ãºlohu (daily_scrape nebo weekly_full_rescan)."""
    if _scheduler is None:
        raise HTTPException(status_code=503, detail="Scheduler nenÃ­ aktivnÃ­")
    job = _scheduler.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' nenalezen")
    _scheduler.pause_job(job_id)
    return {"job_id": job_id, "status": "paused"}


@app.put("/v1/schedule/jobs/{job_id}/resume")
async def resume_scheduled_job(job_id: str):
    """ObnovÃ­ pozastavenou naplÃ¡novanou Ãºlohu."""
    if _scheduler is None:
        raise HTTPException(status_code=503, detail="Scheduler nenÃ­ aktivnÃ­")
    job = _scheduler.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' nenalezen")
    _scheduler.resume_job(job_id)
    return {"job_id": job_id, "status": "resumed"}


@app.put("/v1/schedule/jobs/{job_id}/cron")
async def update_job_cron(job_id: str, cron: str):
    """
    ZmÄ›nÃ­ cron vÃ½raz naplÃ¡novanÃ© Ãºlohy za bÄ›hu.
    FormÃ¡t: '0 3 * * *' (minuta hodina den mÄ›sÃ­c den_tÃ½dne)
    """
    if _scheduler is None:
        raise HTTPException(status_code=503, detail="Scheduler nenÃ­ aktivnÃ­")
    job = _scheduler.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail=f"Job '{job_id}' nenalezen")
    try:
        new_trigger = CronTrigger.from_crontab(cron, timezone="Europe/Prague")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"NeplatnÃ½ cron vÃ½raz: {e}")
    _scheduler.reschedule_job(job_id, trigger=new_trigger)
    updated_job = _scheduler.get_job(job_id)
    return {
        "job_id": job_id,
        "cron": cron,
        "next_run": updated_job.next_run_time.isoformat() if updated_job.next_run_time else None,
    }
