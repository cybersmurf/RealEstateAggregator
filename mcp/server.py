"""
RealEstate MCP Server
======================
Model Context Protocol server pro RealEstateAggregator.
UmoÅ¾Åˆuje AI asistentÅ¯m (Claude Desktop, VS Code Copilot, Cursorâ€¦) volat nÃ¡stroje
pro vyhledÃ¡vÃ¡nÃ­, analÃ½zu a RAG dotazy nad realitnÃ­mi inzerÃ¡ty.

SpuÅ¡tÄ›nÃ­ (stdio â€“ Claude Desktop):
    python server.py

SpuÅ¡tÄ›nÃ­ (HTTP/SSE â€“ Docker, vzdÃ¡lenÃ©):
    TRANSPORT=sse python server.py

Konfigurace Claude Desktop (~/.config/claude/claude_desktop_config.json):
    {
      "mcpServers": {
        "realestate": {
          "command": "python",
          "args": ["/path/to/mcp/server.py"],
          "env": { "API_BASE_URL": "http://localhost:5001" }
        }
      }
    }
"""

import os
import json
import logging
import base64
import io
from PIL import Image
from typing import Optional
import httpx
from fastmcp import FastMCP
from mcp.types import ImageContent, TextContent


def _cap_output(text: str, max_chars: int = 0) -> str:
    """ZkrÃ¡tÃ­ textovÃ½ vÃ½stup nÃ¡stroje na max_chars znakÅ¯, aby se nepÅ™ekroÄil kontext."""
    if max_chars <= 0:
        max_chars = MAX_OUTPUT_CHARS
    if len(text) <= max_chars:
        return text
    truncated = text[:max_chars]
    # Odsekni na poslednÃ­ celÃ½ Å™Ã¡dek
    last_newline = truncated.rfind("\n")
    if last_newline > max_chars // 2:
        truncated = truncated[:last_newline]
    used_pct = len(truncated) * 100 // len(text)
    return (
        truncated
        + f"\n\n---\nâš ï¸ **VÃ½stup zkrÃ¡cen na {MAX_OUTPUT_CHARS:,} znakÅ¯** "
        + f"({used_pct}% z {len(text):,}). "
        + "PouÅ¾ij strÃ¡nkovÃ¡nÃ­ (page=N) pro zobrazenÃ­ dalÅ¡Ã­ho obsahu."
    )


def _resize_image(raw_bytes: bytes, max_width: int = 1200, quality: int = 72) -> bytes:
    """Resize image to max_width keeping aspect ratio, compress to JPEG."""
    try:
        img = Image.open(io.BytesIO(raw_bytes))
        if img.mode in ("RGBA", "P"):
            img = img.convert("RGB")
        w, h = img.size
        if w > max_width:
            new_h = int(h * max_width / w)
            img = img.resize((max_width, new_h), Image.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, format="JPEG", quality=quality, optimize=True)
        return buf.getvalue()
    except Exception:
        return raw_bytes  # fallback: original

# â”€â”€â”€ Konfigurace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:5001")
API_TIMEOUT = float(os.getenv("API_TIMEOUT_SECONDS", "30"))
TRANSPORT = os.getenv("TRANSPORT", "stdio")   # "stdio" nebo "sse"
PORT = int(os.getenv("PORT", "8002"))
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_VISION_MODEL = os.getenv("OLLAMA_VISION_MODEL", "llava:7b")

# Max znakÅ¯ kterÃ© jeden MCP tool vrÃ¡tÃ­ â€“ omezuje vÃ½Å¡i kontextu a kreditÅ¯ Claude
MAX_OUTPUT_CHARS = int(os.getenv("MCP_MAX_OUTPUT_CHARS", "200000"))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("realestate-mcp")

# â”€â”€â”€ MCP server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

mcp = FastMCP(
    name="RealEstate Knowledge Base",
    instructions="""
Jsi asistent specializovanÃ½ na analÃ½zu nemovitostÃ­ z ÄŒeskÃ© republiky.
MÃ¡Å¡ pÅ™Ã­stup k databÃ¡zi realitnÃ­ch inzerÃ¡tÅ¯ (1 200+ aktivnÃ­ch) a uloÅ¾enÃ½m analÃ½zÃ¡m.

DostupnÃ© nÃ¡stroje:
- search_listings: VyhledÃ¡vÃ¡nÃ­ inzerÃ¡tÅ¯ (text + filtry ceny, typu, nabÃ­dky)
- get_listing: DetailnÃ­ informace o konkrÃ©tnÃ­m inzerÃ¡tu (text + metadata + fotky jako URL)
- get_listing_photos: ğŸ“¸ Fotky Z INZERÃTU jako obrÃ¡zky viditelnÃ© v chatu
- get_inspection_photos: ğŸ“· Fotky Z PROHLÃDKY jako obrÃ¡zky viditelnÃ© v chatu
- analyze_inspection_photos: ğŸ” AI analÃ½za fotek z prohlÃ­dky (llava vision) â€“ mÃ­stnost, stav, popis, nedostatky
- analyze_listing_photos: ğŸ” AI analÃ½za fotek z inzerÃ¡tu (llava vision) â€“ pÅ™ehled pÅ™ed prohlÃ­dkou
- get_analyses: ZobrazenÃ­ uloÅ¾enÃ½ch analÃ½z pro inzerÃ¡t
- save_analysis: UloÅ¾enÃ­ novÃ© analÃ½zy textu (automaticky se vygeneruje embedding)
- ask_listing: RAG dotaz nad analÃ½zami konkrÃ©tnÃ­ho inzerÃ¡tu
- ask_general: RAG dotaz pÅ™es vÅ¡echny inzerÃ¡ty
- list_sources: PÅ™ehled aktivnÃ­ch realitnÃ­ch zdrojÅ¯
- get_rag_status: Stav RAG systÃ©mu (poÄty embeddingÅ¯)
""",
)


# â”€â”€â”€ HTTP helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _call_api(method: str, path: str, **kwargs) -> dict | list:
    """ZavolÃ¡ .NET API a vrÃ¡tÃ­ JSON odpovÄ›Ä."""
    url = f"{API_BASE_URL}{path}"
    async with httpx.AsyncClient(timeout=API_TIMEOUT) as client:
        resp = await getattr(client, method)(url, **kwargs)
        resp.raise_for_status()
        return resp.json()


def _fmt_listing(l: dict) -> str:
    """FormÃ¡tuje inzerÃ¡t do ÄitelnÃ©ho textu."""
    price = f"{l.get('price', 0):,.0f} KÄ" if l.get("price") else "Cena neuvedena"
    area = f"{l.get('areaBuiltUp', 0):.0f} mÂ²" if l.get("areaBuiltUp") else ""
    disposition = l.get("disposition", "") or ""
    return (
        f"ğŸ  **{l['title']}**\n"
        f"   ID: `{l['id']}`\n"
        f"   ğŸ“ {l.get('locationText', 'N/A')}  |  ğŸ’° {price}"
        f"  |  {disposition} {area}\n"
        f"   Typ: {l.get('propertyType')} | NabÃ­dka: {l.get('offerType')}"
        f"  |  Zdroj: {l.get('sourceName', l.get('sourceCode', ''))}\n"
        f"   ğŸ”— {l.get('url', '')}"
    )


# â”€â”€â”€ NÃSTROJE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@mcp.tool()
async def search_listings(
    query: str = "",
    property_type: Optional[str] = None,
    offer_type: Optional[str] = None,
    min_price: Optional[float] = None,
    max_price: Optional[float] = None,
    municipality: Optional[str] = None,
    page: int = 1,
    page_size: int = 10,
) -> str:
    """
    VyhledÃ¡ realitnÃ­ inzerÃ¡ty v databÃ¡zi.

    Args:
        query: VolnÃ½ textovÃ½ dotaz (napÅ™. "rodinnÃ½ dÅ¯m Znojmo s bazÃ©nem")
        property_type: Typ nemovitosti: House | Apartment | Land | Cottage | Commercial | Garage | Other
        offer_type: Typ nabÃ­dky: Sale | Rent | Auction
        min_price: MinimÃ¡lnÃ­ cena v KÄ
        max_price: MaximÃ¡lnÃ­ cena v KÄ
        municipality: Obec (napÅ™. "Znojmo", "Å tÃ­tary")
        page: ÄŒÃ­slo strÃ¡nky (default 1)
        page_size: PoÄet vÃ½sledkÅ¯ (max 50, default 10)
    """
    payload = {
        "searchQuery": query or None,
        "propertyType": property_type,
        "offerType": offer_type,
        "minPrice": min_price,
        "maxPrice": max_price,
        "municipality": municipality,
        "page": page,
        "pageSize": min(page_size, 50),
    }
    # OdstraÅˆ None hodnoty
    payload = {k: v for k, v in payload.items() if v is not None}

    result = await _call_api("post", "/api/listings/search", json=payload)

    items = result.get("items", [])
    total = result.get("totalCount", 0)

    if not items:
        return "Nenalezeny Å¾Ã¡dnÃ© inzerÃ¡ty odpovÃ­dajÃ­cÃ­ kritÃ©riÃ­m."

    lines = [f"**Nalezeno {total} inzerÃ¡tÅ¯** (strana {page}):\n"]
    for listing in items:
        lines.append(_fmt_listing(listing))
        lines.append("")

    return _cap_output("\n".join(lines))


@mcp.tool()
async def get_listing(listing_id: str) -> str:
    """
    ğŸ” VrÃ¡tÃ­ KOMPLETNÃ detail inzerÃ¡tu vÄetnÄ› ZÃPISU Z PROHLÃDKY.
    
    Co vracÃ­:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - ğŸ“‹ ZÃPIS Z PROHLÃDKY: plnÃ½ text poznÃ¡mek z osobnÃ­ nÃ¡vÅ¡tÄ›vy
    - ğŸ’° Cena, plocha, dispozice, lokalita
    - ğŸ  Typ nemovitosti + typ nabÃ­dky (prodej/pronÃ¡jem/draÅ¾ba)
    - ğŸŒ GPS + okres + okres katastr
    - ğŸ“¸ FOTKY Z INZERÃTU: seznam vÅ¡ech staÅ¾enÃ©ho fotek
    - ğŸ“· FOTKY Z PROHLÃDKY: vlastnÃ­ fotky nahranÃ© bÄ›hem prohlÃ­dky
    - â˜ï¸ GOOGLE DRIVE ODKAZ: pÅ™Ã­mÃ½ link na sloÅ¾ku s analÃ½zami
    - Status: Visited/Liked/ToVisit/Disliked
    
    TypickÃ© workflow:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. get_listing(id) â†’ pÅ™eÄti si ZÃPIS Z PROHLÃDKY
    2. get_analyses(id) â†’ vidÃ­Å¡ co se uÅ¾ napsalo
    3. VytvoÅ™ novou analÃ½zu
    4. save_analysis(id, content) â†’ uloÅ¾Ã­ se do DB + vytvoÅ™Ã­ embedding
    
    âš¡ KRITICKÃ‰: ZÃ¡pis z prohlÃ­dky je ZCELA ODLIÅ NÃ od popisu na webu!
    Obsahuje osobnÃ­ pozorovÃ¡nÃ­, mÄ›Å™enÃ­, kvalitativnÃ­ posouzenÃ­.
    VÅ¾dy si to pÅ™eÄti PÅ˜ED tvorbou analÃ½zy!

    Args:
        listing_id: UUID inzerÃ¡tu (zÃ­skÃ¡Å¡ ho ze search_listings)
    """
    try:
        listing = await _call_api("get", f"/api/listings/{listing_id}")
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return f"InzerÃ¡t {listing_id} nenalezen."
        raise

    photos = listing.get("photos", [])
    user_state = listing.get("userState") or {}

    result_lines = [
        f"# {listing['title']}",
        f"**ID:** `{listing['id']}`",
        f"**Zdroj:** {listing.get('sourceName', listing.get('sourceCode', ''))}",
        f"**Typ:** {listing.get('propertyType')} | **NabÃ­dka:** {listing.get('offerType')}",
        f"**Cena:** {listing.get('price', 0):,.0f} KÄ" if listing.get("price") else "**Cena:** neuvedena",
        f"**Lokalita:** {listing.get('locationText', 'N/A')}",
    ]

    if listing.get("areaBuiltUp"):
        result_lines.append(f"**Plocha zastavÄ›nÃ¡:** {listing['areaBuiltUp']:.0f} mÂ²")
    if listing.get("areaLand"):
        result_lines.append(f"**Plocha pozemku:** {listing['areaLand']:.0f} mÂ²")
    if listing.get("disposition"):
        result_lines.append(f"**Dispozice:** {listing['disposition']}")
    if listing.get("constructionType"):
        result_lines.append(f"**Konstrukce:** {listing['constructionType']}")
    if listing.get("condition"):
        result_lines.append(f"**Stav:** {listing['condition']}")

    result_lines.append(f"**URL:** {listing.get('sourceUrl') or listing.get('url', '')}")

    # â”€â”€ Google Drive / OneDrive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    drive_url = listing.get("driveFolderUrl")
    drive_inspection_url = listing.get("driveInspectionFolderUrl")
    has_onedrive = listing.get("hasOneDriveExport", False)
    if drive_url or has_onedrive:
        result_lines += ["", "## â˜ï¸ Cloud export"]
        if drive_url:
            result_lines.append(f"**Google Drive sloÅ¾ka:** {drive_url}")
        if drive_inspection_url:
            result_lines.append(f"**Google Drive â€“ fotky z prohlÃ­dky:** {drive_inspection_url}")
        if has_onedrive:
            result_lines.append("**OneDrive:** exportovÃ¡no âœ…")

    # â”€â”€ Stav a zÃ¡pis z prohlÃ­dky â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if user_state:
        status = user_state.get("status", "New")
        notes = user_state.get("notes", "")
        updated = (user_state.get("lastUpdated") or "")[:10]
        result_lines += [
            "",
            "## ğŸ“‹ Stav & zÃ¡pis z prohlÃ­dky",
            f"**Stav:** {status} ({updated})",
        ]
        if notes:
            result_lines += [
                "**PoznÃ¡mky / zÃ¡pis z prohlÃ­dky:**",
                notes,
            ]
        else:
            result_lines.append("_Å½Ã¡dnÃ© poznÃ¡mky._")

    # â”€â”€ Fotky z inzerÃ¡tu â€“ jako URL seznam (Claude si je vyÅ¾Ã¡dÃ¡ pÅ™es get_listing_photos) â”€â”€
    result_lines += ["", f"## ğŸ“¸ Fotky z inzerÃ¡tu ({len(photos)})"]
    if photos:
        for i, p in enumerate(photos):
            url = p.get("storedUrl") or p.get("originalUrl") or ""
            result_lines.append(f"- {url}")
        result_lines.append("")
        result_lines.append(f"ğŸ’¡ Pro zobrazenÃ­ fotek zavolej: `get_listing_photos(listing_id='{listing_id}')`")
    else:
        result_lines.append("_Å½Ã¡dnÃ© fotky._")

    # â”€â”€ Fotky z prohlÃ­dky â€“ poÄet a odkaz na dedicated tool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        insp_photos = await _call_api("get", f"/api/listings/{listing['id']}/inspection-photos")
        if insp_photos:
            result_lines += ["", f"## ğŸ“· Fotky z prohlÃ­dky ({len(insp_photos)} â€“ vlastnÃ­)"]
            result_lines.append(f"ğŸ’¡ Pro zobrazenÃ­ fotek zavolej: `get_inspection_photos(listing_id='{listing['id']}')`")
        else:
            result_lines += ["", "## ğŸ“· Fotky z prohlÃ­dky", "_Å½Ã¡dnÃ© vlastnÃ­ fotky z prohlÃ­dky._"]
    except Exception as e:
        logger.warning(f"Failed to fetch inspection photos: {e}")
        pass  # endpoint neexistuje nebo vrÃ¡til chybu â€“ ignoruj

    # â”€â”€ Popis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    result_lines += [
        "",
        "## Popis",
        listing.get("description", "Bez popisu")[:3000],
    ]
    if listing.get("description", "") and len(listing["description"]) > 3000:
        result_lines.append("_[popis zkrÃ¡cen na 3000 znakÅ¯]_")

    return _cap_output("\n".join(result_lines))


@mcp.tool()
async def get_inspection_photos(listing_id: str, page: int = 1, page_size: int = 10) -> list:
    """
    ğŸ“· VrÃ¡tÃ­ fotky z prohlÃ­dky jako OBRÃZKY (ne URL).
    Claude je vidÃ­ pÅ™Ã­mo v chatu! Fotky jsou automaticky zmenÅ¡eny na max 1200px.

    Args:
        listing_id: UUID inzerÃ¡tu
        page: StrÃ¡nka (zaÄÃ­nÃ¡ 1, default 1)
        page_size: PoÄet fotek na strÃ¡nku (default 10, max 20)
    """
    try:
        photos = await _call_api("get", f"/api/listings/{listing_id}/inspection-photos")
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return [TextContent(type="text", text=f"InzerÃ¡t {listing_id} nenalezen.")]
        raise

    if not photos:
        return [TextContent(type="text", text=f"Pro inzerÃ¡t {listing_id} nejsou uloÅ¾eny Å¾Ã¡dnÃ© vlastnÃ­ fotky z prohlÃ­dky.")]

    page_size = min(max(1, page_size), 20)
    total = len(photos)
    total_pages = (total + page_size - 1) // page_size
    page = max(1, min(page, total_pages))
    start = (page - 1) * page_size
    end = min(start + page_size, total)
    page_photos = photos[start:end]

    result = [TextContent(type="text", text=
        f"**Fotky z prohlÃ­dky** â€“ strÃ¡nka {page}/{total_pages} "
        f"({start+1}â€“{end} z {total} fotek):\n"
        + (f"â¡ï¸ DalÅ¡Ã­: `get_inspection_photos(listing_id='{listing_id}', page={page+1})`" if page < total_pages else "")
    )]

    async with httpx.AsyncClient(timeout=60) as client:
        for i, p in enumerate(page_photos, start + 1):
            filename = p.get('originalFileName', f'photo-{i}.jpg')
            filesize_kb = p.get('fileSizeBytes', 0) // 1024
            url = p.get('url', '')
            result.append(TextContent(type="text", text=f"**{i}. {filename}** (orig. {filesize_kb} KB)"))
            try:
                if url:
                    r = await client.get(url)
                    r.raise_for_status()
                    resized = _resize_image(r.content)
                    b64_data = base64.b64encode(resized).decode()
                    result.append(ImageContent(type="image", data=b64_data, mimeType="image/jpeg"))
            except Exception as e:
                logger.warning(f"Failed to fetch photo {filename}: {e}")
                result.append(TextContent(type="text", text=f"âŒ {url} (selhalo: {e})"))

    return result


@mcp.tool()
async def get_listing_photos(listing_id: str, page: int = 1, page_size: int = 10) -> list:
    """
    ğŸ“¸ VrÃ¡tÃ­ fotky z inzerÃ¡tu jako OBRÃZKY (ne URL).
    Claude je vidÃ­ pÅ™Ã­mo v chatu! Fotky jsou automaticky zmenÅ¡eny na max 1200px.

    Args:
        listing_id: UUID inzerÃ¡tu
        page: StrÃ¡nka (zaÄÃ­nÃ¡ 1, default 1)
        page_size: PoÄet fotek na strÃ¡nku (default 10, max 20)
    """
    try:
        listing = await _call_api("get", f"/api/listings/{listing_id}")
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return [TextContent(type="text", text=f"InzerÃ¡t {listing_id} nenalezen.")]
        raise

    photos = listing.get("photos", [])
    if not photos:
        return [TextContent(type="text", text="Å½Ã¡dnÃ© fotky z inzerÃ¡tu.")]

    page_size = min(max(1, page_size), 20)
    total = len(photos)
    total_pages = (total + page_size - 1) // page_size
    page = max(1, min(page, total_pages))
    start = (page - 1) * page_size
    end = min(start + page_size, total)
    page_photos = photos[start:end]

    result = [TextContent(type="text", text=
        f"**Fotky z inzerÃ¡tu** â€“ strÃ¡nka {page}/{total_pages} "
        f"({start+1}â€“{end} z {total} fotek):\n"
        + (f"â¡ï¸ DalÅ¡Ã­: `get_listing_photos(listing_id='{listing_id}', page={page+1})`" if page < total_pages else "")
    )]

    async with httpx.AsyncClient(timeout=60) as client:
        for i, p in enumerate(page_photos, start + 1):
            url = p.get("storedUrl") or p.get("originalUrl") or ""
            if not url:
                continue
            result.append(TextContent(type="text", text=f"**{i}.**"))
            try:
                r = await client.get(url)
                r.raise_for_status()
                resized = _resize_image(r.content)
                b64_data = base64.b64encode(resized).decode()
                result.append(ImageContent(type="image", data=b64_data, mimeType="image/jpeg"))
            except Exception as e:
                logger.warning(f"Failed to fetch listing photo {url}: {e}")
                result.append(TextContent(type="text", text=f"âŒ {url} (selhalo)"))

    return result


@mcp.tool()
async def analyze_inspection_photos(listing_id: str, page: int = 1, page_size: int = 10, force: bool = False) -> str:
    """
    ğŸ” Analyzuje fotky z prohlÃ­dky pomocÃ­ AI vision modelu (llava:7b).
    VÃ½sledky jsou ULOÅ½ENY DO DB â€“ pÅ™Ã­Å¡tÄ› se naÄtou z cache (rychlÃ©).
    Fotky zpracovÃ¡vÃ¡ po strÃ¡nkÃ¡ch (default 10 fotek/strÃ¡nka).

    Args:
        listing_id: UUID inzerÃ¡tu
        page: StrÃ¡nka fotek (zaÄÃ­nÃ¡ 1)
        page_size: PoÄet fotek na strÃ¡nku (default 10, max 15)
        force: True = pÅ™eanalyzuj i fotky co uÅ¾ majÃ­ popis (default False)
    """
    PROMPT = (
        "Jsi expert na nemovitosti. PopiÅ¡ tuto fotografii z prohlÃ­dky nemovitosti. "
        "OdpovÄ›z VÃHRADNÄš v tomto formÃ¡tu (kaÅ¾dÃ½ bod na novÃ½ Å™Ã¡dek):\n"
        "MÃSTNOST: [typ mÃ­stnosti â€“ kuchynÄ›/obÃ½vacÃ­ pokoj/loÅ¾nice/koupelna/WC/chodba/sklep/garÃ¡Å¾/zahrada/exteriÃ©r/jinÃ©]\n"
        "STAV: [vÃ½bornÃ½/dobrÃ½/prÅ¯mÄ›rnÃ½/Å¡patnÃ½/hrubÃ¡ stavba]\n"
        "POPIS: [1-2 vÄ›ty o tom co vidÃ­Å¡ â€“ materiÃ¡ly, vybavenÃ­, svÄ›tlo, rozmÄ›ry]\n"
        "POZOR: [pÅ™Ã­padnÃ© nedostatky nebo vÄ›ci k Å™eÅ¡enÃ­, nebo 'Å½Ã¡dnÃ©']\n"
        "OdpovÃ­dej Äesky."
    )

    try:
        photos = await _call_api("get", f"/api/listings/{listing_id}/inspection-photos")
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return f"InzerÃ¡t {listing_id} nenalezen."
        raise

    if not photos:
        return f"Pro inzerÃ¡t {listing_id} nejsou uloÅ¾eny Å¾Ã¡dnÃ© fotky z prohlÃ­dky."

    page_size = min(max(1, page_size), 15)
    total = len(photos)
    total_pages = (total + page_size - 1) // page_size
    page = max(1, min(page, total_pages))
    start = (page - 1) * page_size
    end = min(start + page_size, total)
    page_photos = photos[start:end]

    cached_count = sum(1 for p in page_photos if p.get("aiDescription") and not force)
    lines = [
        f"## ğŸ” AI analÃ½za fotek z prohlÃ­dky â€“ strÃ¡nka {page}/{total_pages} ({start+1}â€“{end} z {total})",
        f"Model: `{OLLAMA_VISION_MODEL}` | InzerÃ¡t: `{listing_id}` | ğŸ’¾ Cache: {cached_count}/{len(page_photos)}\n",
    ]

    async with httpx.AsyncClient(timeout=120) as client:
        for i, p in enumerate(page_photos, start + 1):
            photo_id = p.get("id", "")
            filename = p.get("originalFileName", f"photo-{i}.jpg")
            url = p.get("url", "")
            cached = p.get("aiDescription")
            lines.append(f"\n---\n### Fotka {i}: `{filename}`")

            if not url:
                lines.append("_URL chybÃ­ â€“ pÅ™eskoÄeno._")
                continue

            if cached and not force:
                lines.append(f"_(z cache)_\n{cached}")
                continue

            try:
                r = await client.get(url)
                r.raise_for_status()
                resized = _resize_image(r.content, max_width=800, quality=80)
                b64 = base64.b64encode(resized).decode()

                ollama_resp = await client.post(
                    f"{OLLAMA_BASE_URL}/api/chat",
                    json={
                        "model": OLLAMA_VISION_MODEL,
                        "messages": [{"role": "user", "content": PROMPT, "images": [b64]}],
                        "stream": False,
                        "options": {"temperature": 0.1},
                    },
                    timeout=60,
                )
                ollama_resp.raise_for_status()
                answer = ollama_resp.json()["message"]["content"].strip()
                lines.append(answer)

                # UloÅ¾ do DB
                if photo_id:
                    try:
                        await client.patch(
                            f"{API_BASE_URL}/api/listings/{listing_id}/inspection-photos/{photo_id}/ai-description",
                            json={"description": answer},
                            timeout=10,
                        )
                    except Exception as save_err:
                        logger.warning(f"Failed to save ai_description for photo {photo_id}: {save_err}")

            except Exception as e:
                logger.warning(f"Ollama analysis failed for {filename}: {e}")
                lines.append(f"âŒ AnalÃ½za selhala: {e}")

    if page < total_pages:
        lines.append(
            f"\n---\nâ¡ï¸ DalÅ¡Ã­ strÃ¡nka: "
            f"`analyze_inspection_photos(listing_id='{listing_id}', page={page + 1})`"
        )

    return _cap_output("\n".join(lines))


@mcp.tool()
async def analyze_listing_photos(listing_id: str, page: int = 1, page_size: int = 10, force: bool = False) -> str:
    """
    ğŸ” Analyzuje fotky Z INZERÃTU pomocÃ­ AI vision modelu (llava:7b).
    VÃ½sledky jsou ULOÅ½ENY DO DB â€“ pÅ™Ã­Å¡tÄ› se naÄtou z cache (rychlÃ©).
    HodÃ­ se pro rychlÃ½ pÅ™ehled nemovitosti jeÅ¡tÄ› pÅ™ed prohlÃ­dkou.

    Args:
        listing_id: UUID inzerÃ¡tu
        page: StrÃ¡nka fotek (zaÄÃ­nÃ¡ 1)
        page_size: PoÄet fotek na strÃ¡nku (default 10, max 15)
        force: True = pÅ™eanalyzuj i fotky co uÅ¾ majÃ­ popis (default False)
    """
    PROMPT = (
        "Jsi expert na nemovitosti. PopiÅ¡ tuto fotografii z realitnÃ­ho inzerÃ¡tu. "
        "OdpovÄ›z VÃHRADNÄš v tomto formÃ¡tu (kaÅ¾dÃ½ bod na novÃ½ Å™Ã¡dek):\n"
        "MÃSTNOST: [typ mÃ­stnosti â€“ kuchynÄ›/obÃ½vacÃ­ pokoj/loÅ¾nice/koupelna/WC/chodba/sklep/garÃ¡Å¾/zahrada/exteriÃ©r/jinÃ©]\n"
        "STAV: [vÃ½bornÃ½/dobrÃ½/prÅ¯mÄ›rnÃ½/Å¡patnÃ½/hrubÃ¡ stavba]\n"
        "POPIS: [1-2 vÄ›ty o tom co vidÃ­Å¡ â€“ materiÃ¡ly, vybavenÃ­, svÄ›tlo, rozmÄ›ry]\n"
        "POZOR: [pÅ™Ã­padnÃ© nedostatky nebo vÄ›ci k provÄ›Å™enÃ­ na prohlÃ­dce, nebo 'Å½Ã¡dnÃ©']\n"
        "OdpovÃ­dej Äesky."
    )

    try:
        listing = await _call_api("get", f"/api/listings/{listing_id}")
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return f"InzerÃ¡t {listing_id} nenalezen."
        raise

    photos = listing.get("photos", [])
    if not photos:
        return "InzerÃ¡t nemÃ¡ Å¾Ã¡dnÃ© fotky."

    page_size = min(max(1, page_size), 15)
    total = len(photos)
    total_pages = (total + page_size - 1) // page_size
    page = max(1, min(page, total_pages))
    start = (page - 1) * page_size
    end = min(start + page_size, total)
    page_photos = photos[start:end]

    title = listing.get("title", listing_id)
    cached_count = sum(1 for p in page_photos if p.get("aiDescription") and not force)
    lines = [
        f"## ğŸ” AI analÃ½za fotek z inzerÃ¡tu â€“ strÃ¡nka {page}/{total_pages} ({start+1}â€“{end} z {total})",
        f"**{title}** | Model: `{OLLAMA_VISION_MODEL}` | ğŸ’¾ Cache: {cached_count}/{len(page_photos)}\n",
    ]

    async with httpx.AsyncClient(timeout=120) as client:
        for i, p in enumerate(page_photos, start + 1):
            photo_id = p.get("id", "")
            url = p.get("storedUrl") or p.get("originalUrl") or ""
            cached = p.get("aiDescription")
            lines.append(f"\n---\n### Fotka {i}")

            if not url:
                lines.append("_URL chybÃ­ â€“ pÅ™eskoÄeno._")
                continue

            if cached and not force:
                lines.append(f"_(z cache)_\n{cached}")
                continue

            try:
                r = await client.get(url)
                r.raise_for_status()
                resized = _resize_image(r.content, max_width=800, quality=80)
                b64 = base64.b64encode(resized).decode()

                ollama_resp = await client.post(
                    f"{OLLAMA_BASE_URL}/api/chat",
                    json={
                        "model": OLLAMA_VISION_MODEL,
                        "messages": [{"role": "user", "content": PROMPT, "images": [b64]}],
                        "stream": False,
                        "options": {"temperature": 0.1},
                    },
                    timeout=60,
                )
                ollama_resp.raise_for_status()
                answer = ollama_resp.json()["message"]["content"].strip()
                lines.append(answer)

                # UloÅ¾ do DB
                if photo_id:
                    try:
                        await client.patch(
                            f"{API_BASE_URL}/api/listings/{listing_id}/photos/{photo_id}/ai-description",
                            json={"description": answer},
                            timeout=10,
                        )
                    except Exception as save_err:
                        logger.warning(f"Failed to save ai_description for listing photo {photo_id}: {save_err}")

            except Exception as e:
                logger.warning(f"Ollama analysis failed for listing photo {i}: {e}")
                lines.append(f"âŒ AnalÃ½za selhala: {e}")

    if page < total_pages:
        lines.append(
            f"\n---\nâ¡ï¸ DalÅ¡Ã­ strÃ¡nka: "
            f"`analyze_listing_photos(listing_id='{listing_id}', page={page + 1})`"
        )

    return _cap_output("\n".join(lines))


@mcp.tool()
async def analyze_tovisit_listings(
    force: bool = False,
    max_photos_per_listing: int = 10,
) -> str:
    """
    ğŸ  Analyzuje fotky VÅ ECH inzerÃ¡tÅ¯ oznaÄenÃ½ch 'K nÃ¡vÅ¡tÄ›vÄ›' pomocÃ­ AI vision.

    SpouÅ¡tÄ›j ruÄnÄ› pÅ™ed plÃ¡novanÃ½mi prohlÃ­dkami â€“ pÅ™ipravÃ­ si popis
    kaÅ¾dÃ© fotky z inzerÃ¡tu, aby byl detail dostupnÃ½ bez ÄekÃ¡nÃ­.
    VÃ½sledky se uklÃ¡dajÃ­ do DB â€“ pÅ™Ã­Å¡tÄ› se naÄtou z cache (rychlÃ©).

    Fotky z prohlÃ­dky (vlastnÃ­ upload) analyzuj zvlÃ¡Å¡Å¥ pÅ™es
    `analyze_inspection_photos`.

    Args:
        force: True = pÅ™eanalyzuj i fotky co uÅ¾ majÃ­ popis (default False)
        max_photos_per_listing: Max fotek na inzerÃ¡t (default 10, max 20)
    """
    PROMPT = (
        "Jsi expert na nemovitosti. PopiÅ¡ tuto fotografii z realitnÃ­ho inzerÃ¡tu. "
        "OdpovÄ›z VÃHRADNÄš v tomto formÃ¡tu (kaÅ¾dÃ½ bod na novÃ½ Å™Ã¡dek):\n"
        "MÃSTNOST: [typ mÃ­stnosti â€“ kuchynÄ›/obÃ½vacÃ­ pokoj/loÅ¾nice/koupelna/WC/chodba/sklep/garÃ¡Å¾/zahrada/exteriÃ©r/jinÃ©]\n"
        "STAV: [vÃ½bornÃ½/dobrÃ½/prÅ¯mÄ›rnÃ½/Å¡patnÃ½/hrubÃ¡ stavba]\n"
        "POPIS: [1-2 vÄ›ty o tom co vidÃ­Å¡ â€“ materiÃ¡ly, vybavenÃ­, svÄ›tlo, rozmÄ›ry]\n"
        "POZOR: [pÅ™Ã­padnÃ© nedostatky nebo vÄ›ci k provÄ›Å™enÃ­ na prohlÃ­dce, nebo 'Å½Ã¡dnÃ©']\n"
        "OdpovÃ­dej Äesky."
    )

    max_photos_per_listing = min(max(1, max_photos_per_listing), 20)

    # 1. NaÄti vÅ¡echny ToVisit inzerÃ¡ty
    result = await _call_api(
        "post",
        "/api/listings/search",
        json={"userStatus": "ToVisit", "pageSize": 100, "page": 1},
    )
    listings = result.get("items", [])
    total_listings = result.get("totalCount", 0)

    if not listings:
        return "âœ… Å½Ã¡dnÃ© inzerÃ¡ty oznaÄenÃ© 'K nÃ¡vÅ¡tÄ›vÄ›' nebyly nalezeny."

    # Pokud je strÃ¡nek vÃ­ce, naÄti vÅ¡echny
    if total_listings > 100:
        all_listings = list(listings)
        page = 2
        while len(all_listings) < total_listings:
            batch = await _call_api(
                "post",
                "/api/listings/search",
                json={"userStatus": "ToVisit", "pageSize": 100, "page": page},
            )
            batch_items = batch.get("items", [])
            if not batch_items:
                break
            all_listings.extend(batch_items)
            page += 1
        listings = all_listings

    lines = [
        f"## ğŸ  AI analÃ½za fotek inzerÃ¡tÅ¯ 'K nÃ¡vÅ¡tÄ›vÄ›'",
        f"**Celkem inzerÃ¡tÅ¯:** {len(listings)} | **Model:** `{OLLAMA_VISION_MODEL}`",
        f"**Fotek max / inzerÃ¡t:** {max_photos_per_listing} | **force:** {force}\n",
    ]

    total_photos = 0
    total_cached = 0
    total_analyzed = 0
    total_failed = 0

    async with httpx.AsyncClient(timeout=120) as client:
        for listing_summary in listings:
            listing_id = listing_summary.get("id", "")
            title = listing_summary.get("title", listing_id)

            # NaÄti detail inzerÃ¡tu (obsahuje fotky s aiDescription)
            try:
                listing = await _call_api("get", f"/api/listings/{listing_id}")
            except Exception as e:
                lines.append(f"\n### âŒ {title}\nChyba naÄtenÃ­: {e}")
                continue

            photos = listing.get("photos", [])
            if not photos:
                lines.append(f"\n### â¬œ {title}\n_Bez fotek._")
                continue

            # Omez poÄet fotek
            photos_to_process = photos[:max_photos_per_listing]
            cached = sum(1 for p in photos_to_process if p.get("aiDescription") and not force)
            to_analyze = len(photos_to_process) - cached

            lines.append(
                f"\n### ğŸ¡ {title}\n"
                f"Fotek: {len(photos)} | ZpracovÃ¡vÃ¡m: {len(photos_to_process)} "
                f"| ğŸ’¾ Cache: {cached} | ğŸ” NovÃ©: {to_analyze}"
            )

            listing_analyzed = 0
            listing_failed = 0

            for i, p in enumerate(photos_to_process, 1):
                photo_id = p.get("id", "")
                url = p.get("storedUrl") or p.get("originalUrl") or ""
                cached_desc = p.get("aiDescription")

                total_photos += 1

                if not url:
                    continue

                if cached_desc and not force:
                    total_cached += 1
                    continue

                try:
                    r = await client.get(url)
                    r.raise_for_status()
                    resized = _resize_image(r.content, max_width=800, quality=80)
                    b64 = base64.b64encode(resized).decode()

                    ollama_resp = await client.post(
                        f"{OLLAMA_BASE_URL}/api/chat",
                        json={
                            "model": OLLAMA_VISION_MODEL,
                            "messages": [{"role": "user", "content": PROMPT, "images": [b64]}],
                            "stream": False,
                            "options": {"temperature": 0.1},
                        },
                        timeout=90,
                    )
                    ollama_resp.raise_for_status()
                    answer = ollama_resp.json()["message"]["content"].strip()
                    listing_analyzed += 1
                    total_analyzed += 1

                    # UloÅ¾ do DB
                    if photo_id:
                        try:
                            await client.patch(
                                f"{API_BASE_URL}/api/listings/{listing_id}/photos/{photo_id}/ai-description",
                                json={"description": answer},
                                timeout=10,
                            )
                        except Exception as save_err:
                            logger.warning(
                                f"Failed to save ai_description for photo {photo_id}: {save_err}"
                            )

                except Exception as e:
                    logger.warning(f"Ollama analysis failed for listing {listing_id} photo {i}: {e}")
                    listing_failed += 1
                    total_failed += 1

            status_icon = "âœ…" if listing_failed == 0 else "âš ï¸"
            lines.append(
                f"{status_icon} Hotovo: {listing_analyzed} novÃ½ch, "
                f"{cached} z cache, {listing_failed} chyb"
            )

    lines.append(
        f"\n---\n## ğŸ“Š CelkovÃ½ pÅ™ehled\n"
        f"- ğŸ“· Fotek zpracovÃ¡no: **{total_photos}**\n"
        f"- ğŸ” NovÄ› analyzovÃ¡no: **{total_analyzed}**\n"
        f"- ğŸ’¾ Z cache: **{total_cached}**\n"
        f"- âŒ Chyb: **{total_failed}**"
    )

    return _cap_output("\n".join(lines))


@mcp.tool()
async def get_analyses(listing_id: str) -> str:
    """
    ğŸ“Š VrÃ¡tÃ­ VÅ ECHNY uloÅ¾enÃ© analÃ½zy pro konkrÃ©tnÃ­ inzerÃ¡t.
    
    Obsahuje:
    - PlnÃ½ obsah kaÅ¾dÃ© analÃ½zy (bez zkrÃ¡cenÃ­!)
    - Nadpis a zdroj (claude | mcp | manual | ai | ...)
    - DÃ¡tu vytvoÅ™enÃ­ analÃ½zy
    - Status embeddingu (zda je prohledÃ¡vatelnÃ¡ pÅ™es RAG)
    - ID analÃ½zy (pro pÅ™Ã­padnÃ© smazÃ¡nÃ­)
    
    DÅ®LEÅ½ITÃ‰: Jsou tu VÅ ECHNY analÃ½zy kterÃ© kdy byly uloÅ¾eny, 
    ne jen ty nejnovÄ›jÅ¡Ã­! Skrz historii vidÃ­Å¡ evoluci posouzenÃ­.

    Args:
        listing_id: UUID inzerÃ¡tu (zÃ­skÃ¡Å¡ ho ze search_listings nebo get_listing)
    """
    try:
        analyses = await _call_api("get", f"/api/listings/{listing_id}/analyses")
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return f"InzerÃ¡t {listing_id} nenalezen."
        raise

    if not analyses:
        return f"Pro inzerÃ¡t {listing_id} nejsou uloÅ¾eny Å¾Ã¡dnÃ© analÃ½zy."

    lines = [f"**{len(analyses)} analÃ½z** pro inzerÃ¡t `{listing_id}`:\n"]
    for a in analyses:
        emb = "âœ… embedding" if a.get("hasEmbedding") else "âŒ bez embeddingu"
        lines.append(
            f"### [{a.get('title') or 'bez nÃ¡zvu'}] â€“ {a.get('source', 'manual')} â€“ {emb}"
        )
        lines.append(f"*{a.get('createdAt', '')[:10]}*")
        lines.append(f"`ID: {a['id']}`")
        content = a.get("content", "")
        lines.append(content)  # plnÃ½ obsah bez zkrÃ¡cenÃ­
        lines.append("")

    return _cap_output("\n".join(lines))


@mcp.tool()
async def save_analysis(
    listing_id: str,
    content: str,
    title: Optional[str] = None,
    source: str = "claude",
) -> str:
    """
    ğŸ’¾ UloÅ¾Ã­ NOVOU analÃ½zu inzerÃ¡tu do databÃ¡ze.
    
    Automaticky se vygeneruje pgvector embedding (pokud je OpenAI klÃ­Ä nakonfigurovÃ¡n),
    takÅ¾e text bude prohledatelnÃ½ pÅ™es RAG a bude dostupnÃ½ pro budoucÃ­ dotazy.
    
    Workflow:
    1. Zavolej get_listing() â†’ pÅ™eÄti si vÅ¡echna data (ZÃPIS Z PROHLÃDKY!)
    2. Zavolej get_analyses() â†’ vidÃ­Å¡ vÅ¡echny dosavadnÃ­ analÃ½zy
    3. VytvoÅ™ novou analÃ½zu v Markdown formÃ¡tu
    4. Zavolej save_analysis() â†’ uloÅ¾Ã­ se a bude prohledÃ¡vatelnÃ¡
    
    POZOR: UloÅ¾enÃ© analÃ½zy jsou vidÄ›t vÅ¡em nÃ¡strojÅ¯m (RAG dotazovÃ¡nÃ­, 
    dalÅ¡Ã­ analÃ½zy, UI). NeuklÃ¡dej sem draft Äi nejistÃ© vÄ›ci!

    Args:
        listing_id: UUID inzerÃ¡tu
        content: PlnÃ½ text analÃ½zy (markdown, plain text â€“ libovolnÃ¡ dÃ©lka)
        title: VolitelnÃ½ nadpis (napÅ™. "AnalÃ½za z prohlÃ­dky 26.2.2026")
        source: PÅ¯vod: "claude" (default) | "mcp" | "manual" | "ai" | "perplexity"
    """
    payload = {
        "content": content,
        "title": title,
        "source": source,
    }
    try:
        result = await _call_api(
            "post", f"/api/listings/{listing_id}/analyses", json=payload
        )
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return f"InzerÃ¡t {listing_id} nenalezen."
        raise

    emb_status = "âœ… embedding vygenerovÃ¡n" if result.get("hasEmbedding") else "âš ï¸ bez embeddingu (OpenAI nenastaveno)"
    return (
        f"âœ… AnalÃ½za uloÅ¾ena\n"
        f"ID: `{result['id']}`\n"
        f"InzerÃ¡t: `{listing_id}`\n"
        f"Zdroj: {result.get('source')}\n"
        f"Embedding: {emb_status}\n"
        f"DÃ©lka: {len(content)} znakÅ¯"
    )


@mcp.tool()
async def ask_listing(
    listing_id: str,
    question: str,
    top_k: int = 5,
) -> str:
    """
    PoloÅ¾Ã­ RAG dotaz nad uloÅ¾enÃ½mi analÃ½zami konkrÃ©tnÃ­ho inzerÃ¡tu.
    PouÅ¾ije pgvector pro nalezenÃ­ nejrelevantnÄ›jÅ¡Ã­ch ÄÃ¡stÃ­ analÃ½z a poÅ¡le je jako kontext do OpenAI.

    Args:
        listing_id: UUID inzerÃ¡tu
        question: OtÃ¡zka v pÅ™irozenÃ©m jazyce (Äesky nebo anglicky)
        top_k: PoÄet nejpodobnÄ›jÅ¡Ã­ch analÃ½z pouÅ¾itÃ½ch jako kontext (default 5)
    """
    payload = {"question": question, "topK": top_k}
    try:
        result = await _call_api(
            "post", f"/api/listings/{listing_id}/ask", json=payload
        )
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            return f"InzerÃ¡t {listing_id} nenalezen."
        raise

    answer = result.get("answer", "")
    sources = result.get("sources", [])
    has_emb = result.get("hasEmbeddings", False)

    lines = [answer, ""]
    if sources:
        lines.append(f"---\n*PouÅ¾itÃ© zdroje ({len(sources)}):*")
        for s in sources:
            sim = s.get("similarity", 0)
            lines.append(
                f"- [{s.get('title') or 'analÃ½za'}] "
                f"{s.get('source')} | podobnost: {sim:.2%} | `{s['analysisId']}`"
            )
    if not has_emb:
        lines.append("\nâš ï¸ PodobnostnÃ­ vyhledÃ¡vÃ¡nÃ­ nebylo pouÅ¾ito (analyzy nemajÃ­ embedding nebo OpenAI nenÃ­ nakonfigurovÃ¡no).")

    return _cap_output("\n".join(lines))


@mcp.tool()
async def ask_general(
    question: str,
    top_k: int = 5,
) -> str:
    """
    PoloÅ¾Ã­ RAG dotaz pÅ™es analÃ½zy VÅ ECH inzerÃ¡tÅ¯ v databÃ¡zi.
    IdeÃ¡lnÃ­ pro otÃ¡zky jako "kterÃ½ inzerÃ¡t mÃ¡ nejvÄ›tÅ¡Ã­ pozemek pod 2M KÄ?" nebo
    "porovnej vÃ½hody inzerÃ¡tÅ¯ z Moravy".

    Args:
        question: OtÃ¡zka v pÅ™irozenÃ©m jazyce
        top_k: PoÄet nejpodobnÄ›jÅ¡Ã­ch analÃ½z z celÃ© databÃ¡ze (default 5)
    """
    payload = {"question": question, "topK": top_k}
    result = await _call_api("post", "/api/rag/ask", json=payload)

    answer = result.get("answer", "")
    sources = result.get("sources", [])

    lines = [answer, ""]
    if sources:
        lines.append(f"---\n*PouÅ¾itÃ© zdroje ({len(sources)}):*")
        for s in sources:
            sim = s.get("similarity", 0)
            lines.append(
                f"- [{s.get('title') or 'analÃ½za'}] "
                f"inzerÃ¡t `{s.get('listingId', s.get('analysisId'))}` | "
                f"podobnost: {sim:.2%}"
            )

    return _cap_output("\n".join(lines))


@mcp.tool()
async def list_sources() -> str:
    """
    VrÃ¡tÃ­ seznam aktivnÃ­ch realitnÃ­ch zdrojÅ¯ (portÃ¡lÅ¯) a poÄty jejich inzerÃ¡tÅ¯.
    """
    sources = await _call_api("get", "/api/sources")

    if not sources:
        return "Å½Ã¡dnÃ© aktivnÃ­ zdroje nenalezeny."

    lines = [f"**{len(sources)} aktivnÃ­ch zdrojÅ¯:**\n"]
    for s in sources:
        lines.append(
            f"- **{s.get('name', s.get('code'))}** (`{s.get('code')}`)"
            f" â€“ {s.get('listingCount', '?')} inzerÃ¡tÅ¯ | {s.get('baseUrl', '')}"
        )
    return "\n".join(lines)


@mcp.tool()
async def get_rag_status() -> str:
    """
    VrÃ¡tÃ­ stav RAG systÃ©mu: poÄty analÃ½z, embeddingÅ¯ a zda je OpenAI nakonfigurovÃ¡no.
    """
    status = await _call_api("get", "/api/rag/status")

    configured = status.get("openAiConfigured", False)
    emb_icon = "âœ…" if configured else "âŒ"

    return (
        f"## RAG Status\n"
        f"{emb_icon} **OpenAI:** {'nakonfigurovÃ¡no' if configured else 'NENÃ nakonfigurovÃ¡no (embeddingy nefungujÃ­)'}\n"
        f"ğŸ“ **Celkem analÃ½z:** {status.get('totalAnalyses', 0)}\n"
        f"ğŸ”¢ **S embeddingem:** {status.get('withEmbedding', 0)}\n"
        f"âš ï¸ **Bez embeddingu:** {status.get('withoutEmbedding', 0)}\n"
        f"ğŸ  **InzerÃ¡tÅ¯ s analÃ½zou:** {status.get('listingsWithAnalyses', 0)}"
    )


@mcp.tool()
async def embed_description(listing_id: str) -> str:
    """
    Embeduje popis inzerÃ¡tu jako 'auto' analÃ½zu do RAG znalostnÃ­ bÃ¡ze.
    IdempotentnÃ­ â€“ pÅ™eskoÄÃ­ pokud embedding jiÅ¾ existuje.
    Je nutnÃ© spustit jednou pÅ™ed prvnÃ­m dotazem (ask_listing).
    """
    result = await _call_api("post", f"/api/listings/{listing_id}/embed-description")
    if result.get("alreadyExists"):
        return "âœ… Popis inzerÃ¡tu je jiÅ¾ embedovÃ¡n."
    analysis = result
    has_emb = analysis.get("hasEmbedding", False)
    emb_icon = "âœ…" if has_emb else "âš ï¸"
    return (
        f"{emb_icon} Popis embedovÃ¡n jako analÃ½za\n"
        f"ID: {analysis.get('id')}\n"
        f"Titulek: {analysis.get('title')}\n"
        f"Embedding: {'ano' if has_emb else 'ne (Ollama nedostupnÃ¡?)'}"
    )


@mcp.tool()
async def bulk_embed_descriptions(limit: int = 100) -> str:
    """
    Batch embed popisÅ¯ inzerÃ¡tÅ¯ bez 'auto' analÃ½zy.
    VhodnÃ© pro inicializaci knowledge base.
    limit: maximÃ¡lnÃ­ poÄet inzerÃ¡tÅ¯ ke zpracovÃ¡nÃ­ (vÃ½chozÃ­ 100).
    """
    result = await _call_api("post", "/api/rag/embed-descriptions", json={"limit": limit})
    processed = result.get("processed", 0)
    return f"âœ… ZpracovÃ¡no {processed} inzerÃ¡tÅ¯ ({limit} max limit).\n\n{result.get('message', '')}"


# â”€â”€â”€ Entrypoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    if TRANSPORT == "sse":
        import asyncio
        logger.info("Starting MCP server in SSE mode on %s:%d", "0.0.0.0", PORT)
        asyncio.run(mcp.run_http_async(transport="sse", host="0.0.0.0", port=PORT))
    else:
        logger.info("Starting MCP server in stdio mode (API: %s)", API_BASE_URL)
        mcp.run()
